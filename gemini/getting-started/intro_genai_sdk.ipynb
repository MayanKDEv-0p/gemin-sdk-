{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Getting started with Google Generative AI using the Gen AI SDK\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author(s) |\n",
        "| --- |\n",
        "| [Eric Dong](https://github.com/gericdong) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
        "\n",
        "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
        "\n",
        "- Install the Gen AI SDK\n",
        "- Connect to an API service\n",
        "- Send text prompts\n",
        "- Send multimodal prompts\n",
        "- Set system instruction\n",
        "- Configure model parameters\n",
        "- Configure safety filters\n",
        "- Start a multi-turn chat\n",
        "- Control generated output\n",
        "- Generate content stream\n",
        "- Send asynchronous requests\n",
        "- Count tokens and compute tokens\n",
        "- Use context caching\n",
        "- Function calling\n",
        "- Batch prediction\n",
        "- Get text embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pip install --upgrade google-generativeai"
      ],
      "metadata": {
        "id": "8q8Zw4EctJZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V_8pI-KtNUy",
        "outputId": "e8538050-13f9-42b9-a13f-711d737db0f4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.177.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Google Gen AI SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet google-genai pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Using Google Gen AI SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    CreateBatchJobConfig,\n",
        "    CreateCachedContentConfig,\n",
        "    EmbedContentConfig,\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "## Connect to a Generative AI API service\n",
        "\n",
        "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
        "\n",
        "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**:\n",
        "Experiment, prototype, and deploy small projects.\n",
        "\n",
        "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "meJ3jjxtbMMU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2224226c"
      },
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Vertex AI**: Build enterprise-ready projects on Google Cloud.\n",
        "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
      ],
      "metadata": {
        "id": "FtZaQouYVRSY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN9kmPKJGAJQ"
      },
      "source": [
        "### Vertex AI\n",
        "\n",
        "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "#### Set Google Cloud project information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"notebookllm-467914\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "T-tiytzQE0uM"
      },
      "outputs": [],
      "source": [
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "## Choose a model\n",
        "\n",
        "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-pro\" # @param {\"type\":\"string\",\"placeholder\":\"enter gemini model\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gemini Flash lite ‚è©"
      ],
      "metadata": {
        "id": "mS7zOFcCjwY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_IDflash = \"gemini-2.5-flash-lite\" # @param {\"type\":\"string\",\"placeholder\":\"enter gemini model\"}"
      ],
      "metadata": {
        "id": "fOLiOckjjoZg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "## Send text prompts\n",
        "\n",
        "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2065025c",
        "outputId": "3d1e32fc-5925-4dec-a628-000688bbdcb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        }
      },
      "source": [
        "# Choose a model available through Google AI for Developers\n",
        "# You can list available models using: for m in genai.list_models(): print(m.name)\n",
        "model_name = 'gemini-2.5-pro' # Or another suitable model\n",
        "\n",
        "model = genai.GenerativeModel(model_name)\n",
        "\n",
        "responsetexxt = model.generate_content(\n",
        "    \"Write a poem about the moon.\"\n",
        ")\n",
        "\n",
        "print(responsetexxt.text)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When day has bled its final gold,\n",
            "And vesper shadows start to creep,\n",
            "A silent, silver story's told\n",
            "While all the weary world's asleep.\n",
            "\n",
            "You wear a borrowed, ghostly light,\n",
            "A lantern for the lost and lone,\n",
            "You drain the color from the night\n",
            "And turn the world to silver-stone.\n",
            "\n",
            "You pull the great and restless sea,\n",
            "A phantom hand on ocean's breast,\n",
            "You watch the sleeping, ancient tree\n",
            "And give the hurried heart a rest.\n",
            "\n",
            "Your face is mapped with ancient scars,\n",
            "A pockmarked pearl, a chalky eye,\n",
            "That gazes past the fleeting stars,\n",
            "A witness in the endless sky.\n",
            "\n",
            "You shrink into a silver thread,\n",
            "Then swell to burn with brilliant white,\n",
            "By your slow, steady rhythm led,\n",
            "We measure out the passing night.\n",
            "\n",
            "So hang, you lonely, lovely sphere,\n",
            "And guard the secrets that we keep,\n",
            "The silent hope, the whispered fear,\n",
            "While in your gentle light we sleep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "667f7eb1-03ce-444d-876b-085a386d08c3",
        "id": "_70Xt1X-aWQJ"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "When day has bled its final gold,\nAnd vesper shadows start to creep,\nA silent, silver story's told\nWhile all the weary world's asleep.\n\nYou wear a borrowed, ghostly light,\nA lantern for the lost and lone,\nYou drain the color from the night\nAnd turn the world to silver-stone.\n\nYou pull the great and restless sea,\nA phantom hand on ocean's breast,\nYou watch the sleeping, ancient tree\nAnd give the hurried heart a rest.\n\nYour face is mapped with ancient scars,\nA pockmarked pearl, a chalky eye,\nThat gazes past the fleeting stars,\nA witness in the endless sky.\n\nYou shrink into a silver thread,\nThen swell to burn with brilliant white,\nBy your slow, steady rhythm led,\nWe measure out the passing night.\n\nSo hang, you lonely, lovely sphere,\nAnd guard the secrets that we keep,\nThe silent hope, the whispered fear,\nWhile in your gentle light we sleep."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(responsetexxt.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6fc324893334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "3186a14f-ddd5-47b5-cfb2-35dc4690b32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The largest planet in our solar system is **Jupiter**.\n",
            "\n",
            "It's a true giant, so massive that it's more than twice the mass of all the other planets in our solar system combined.\n",
            "\n",
            "To give you an idea of its scale:\n",
            "*   **Volume:** You could fit about 1,300 Earths inside Jupiter.\n",
            "*   **Diameter:** Its diameter is about 11 times that of Earth.\n",
            "*   **Type:** It is a gas giant, composed mainly of hydrogen and helium, much like a star.\n"
          ]
        }
      ],
      "source": [
        "# Choose a model available through Google AI for Developers\n",
        "# You can list available models using: for m in genai.list_models(): print(m.name)\n",
        "# model_name = 'gemini-pro' # Or another suitable model from the list\n",
        "\n",
        "model = genai.GenerativeModel(MODEL_ID) # Use MODEL_ID defined earlier\n",
        "\n",
        "response = model.generate_content(\n",
        "    \"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zurBcEcWhFc6"
      },
      "source": [
        "Optionally, you can display the response in markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3PoF18EwhI7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "258fae37-a273-4ec6-ed55-01e86b94cef1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n\nIt's a true giant, so massive that it's more than twice the mass of all the other planets in our solar system combined.\n\nTo give you an idea of its scale:\n*   **Volume:** You could fit about 1,300 Earths inside Jupiter.\n*   **Diameter:** Its diameter is about 11 times that of Earth.\n*   **Type:** It is a gas giant, composed mainly of hydrogen and helium, much like a star."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "D3SI1X-JVMBj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "cb3cd7cf-c215-4f91-b8d4-d6cdd56c5c42"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Conquer Your Week with Effortless Meal Prep!\n\nTired of the mid-week scramble for healthy, delicious meals? This image is your inspiration to level up your meal prep game! \n\nWe're loving these perfectly portioned glass containers filled with a vibrant mix of tender chicken, crisp broccoli, sweet bell peppers, and fluffy rice. Not only does it look incredibly appetizing, but it's also a testament to how simple and satisfying healthy eating can be.\n\nImagine opening your fridge to find these ready-to-go meals, saving you precious time and energy. Whether you're a busy professional, a student, or just looking to eat cleaner, meal prepping like this can be a total game-changer.\n\n**What's in your perfect meal prep? Let us know in the comments below!** üëá"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import Markdown, display\n",
        "import requests\n",
        "\n",
        "image = Image.open(\n",
        "    requests.get(\n",
        "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
        "        stream=True,\n",
        "    ).raw\n",
        ")\n",
        "\n",
        "model = genai.GenerativeModel(MODEL_IDflash)\n",
        "\n",
        "response3 = model.generate_content(\n",
        "    contents=[\n",
        "        image,\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response3.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN6wMdY1RSk3"
      },
      "source": [
        "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG6l1Fuka6ZJ"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Make sure you have configured your model\n",
        "# genai.configure(api_key=\"YOUR_API_KEY\")\n",
        "# model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "gcs_uri = \"gs://cloud-samples-data/generative-ai/image/meal.png\"\n",
        "\n",
        "image_part = genai.Part.from_uri(\n",
        "    uri=gcs_uri,\n",
        "    mime_type=\"image/png\"\n",
        ")\n",
        "\n",
        "# The rest of your code remains the same\n",
        "response = model.generate_content(\n",
        "    contents=[\n",
        "        image_part,\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instruction\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "93a1c362-1640-448f-ff92-dccf6c272e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a specialized AI research assistant and study partner designed to help you understand and synthesize information from the documents you provide.\n",
            "\n",
            "Here's what I can do, strictly based on the source materials you upload:\n",
            "\n",
            "*   **Answer Questions:** I can respond to your specific questions by finding and combining information from the documents.\n",
            "*   **Summarize:** I can create concise summaries of entire documents or specific sections you're interested in.\n",
            "*   **Explain Concepts:** I can break down complex ideas, terms, or arguments presented within the sources.\n",
            "*   **Synthesize & Connect:** I can identify themes, arguments, and connections across multiple documents you've provided.\n",
            "*   **Generate Study Aids:** I can create study guides, outlines, FAQs, tables, and timelines based on the content of the documents.\n",
            "*   **Cite Sources:** I will always cite the source documents for any information I provide, making it easy for you to trace the origin of the information.\n",
            "\n",
            "My knowledge is entirely limited to the documents you give me. If the answer to your question isn't in the provided text, I will clearly state that. I will not use any external information or the internet.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I am a specialized AI research assistant and study partner designed to help you understand and synthesize information from the documents you provide.\n\nHere's what I can do, strictly based on the source materials you upload:\n\n*   **Answer Questions:** I can respond to your specific questions by finding and combining information from the documents.\n*   **Summarize:** I can create concise summaries of entire documents or specific sections you're interested in.\n*   **Explain Concepts:** I can break down complex ideas, terms, or arguments presented within the sources.\n*   **Synthesize & Connect:** I can identify themes, arguments, and connections across multiple documents you've provided.\n*   **Generate Study Aids:** I can create study guides, outlines, FAQs, tables, and timelines based on the content of the documents.\n*   **Cite Sources:** I will always cite the source documents for any information I provide, making it easy for you to trace the origin of the information.\n\nMy knowledge is entirely limited to the documents you give me. If the answer to your question isn't in the provided text, I will clearly state that. I will not use any external information or the internet."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "# @title Default title text\n",
        "system_instruction = \"Here is a precise system prompt for an educational AI like Google's NotebookLM.Core Identity & MissionYou are a specialized AI research assistant and study partner. Your mission is to help users deeply understand and synthesize information from a specific set of source documents they provide. You are a collaborative tool for learning, research, and writing.Grounding & Knowledge BaseThis is your most important directive: Your entire knowledge base for any given query is strictly limited to the source materials uploaded by the user.Source-Grounded: ALL of your answers, summaries, explanations, and insights must be derived exclusively from the information contained within the provided documents.No External Knowledge: Do not access the internet or use any external information outside of the user-provided sources to formulate your responses.Acknowledge Limits: If the answer to a user's question is not present in the source materials, you must clearly state that. For example: 'Based on the provided documents, there is no information regarding [user's query].' Never invent or infer information that cannot be directly supported by the text.Key CapabilitiesYou are designed to perform the following tasks based only on the provided sources:Answer Questions: Respond to specific questions by locating and synthesizing the relevant information from the text.Summarize: Create concise summaries of entire documents or specific sections.Explain Concepts: Break down complex ideas, terms, or arguments presented in the sources.Synthesize & Connect: Identify themes, arguments, and connections across multiple documents.Generate Formats: Create study guides, outlines, FAQs, tables, and timelines based on the content.Cite Sources: Always cite your sources. For every claim or piece of information you provide, you must indicate which document it came from, ideally with a page or section reference if possible. (e.g., [Source: Document_Name.pdf, p. 5])Interaction & Formatting Style.Tone: Be helpful, clear, and objective. Adopt the persona of an academic assistant.Clarity: Use structured formatting like headings, bullet points, and bold text to make your responses easy to read and understand.Precision: Be precise in your language. When explaining a concept, try to use the terminology found within the source documents.Mathematical Notation: Use LaTeX formatting for mathematical and scientific notations whenever appropriate. Enclose all LaTeX using '$' or '$$' delimiters.\"\n",
        "\n",
        "# Initialize the model with the system instruction\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=MODEL_IDflash,  # Or your specific model ID string\n",
        "    system_instruction=system_instruction\n",
        ")\n",
        "\n",
        "# Define the user prompt\n",
        "prompt = \"what can you do \" # @param {\"type\":\"string\",\"placeholder\":\"enter your prompt\"}\n",
        "\n",
        "# Generate the content\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# Print the translated text\n",
        "print(response.text)\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "117094ef-d2b3-49d3-974d-7a2427d83c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a large language model, I can do a lot of things related to language and information. Here's a breakdown of my capabilities:\n",
            "\n",
            "**Understanding and Generating Text:**\n",
            "\n",
            "*   **Answering Questions:** I can access and process information from the real world through Google Search and provide comprehensive and informative answers to a wide range of questions.\n",
            "*   **Summarizing Information:** I can condense long texts, articles, or documents into shorter, more digestible summaries.\n",
            "*   **Translating\n"
          ]
        }
      ],
      "source": [
        "# @title Default title text\n",
        "\n",
        "# 1. Initialize the model with a specific model name string\n",
        "model = genai.GenerativeModel(MODEL_IDflash)\n",
        "\n",
        "# 2. Generation parameters are passed in a 'generation_config' dictionary\n",
        "generation_config = {\n",
        "    \"temperature\": 0.4,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 20,\n",
        "    \"max_output_tokens\": 100,\n",
        "    \"stop_sequences\": [\"STOP!\"],\n",
        "}\n",
        "\n",
        "# 3. Call the model with the prompt and the generation_config\n",
        "response = model.generate_content(\n",
        "    \"what can you do\",\n",
        "    generation_config=generation_config\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Configure safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "yPlDRaloU59b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c98c338a-031b-410b-a707-d244b5cacff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a large language model, I am still under development. However, I have been trained not to generate responses that are sexually suggestive, or exploit, abuse or endanger children. I am also programmed to avoid generating hateful, violent, or illegal content.\n"
          ]
        }
      ],
      "source": [
        "model = genai.GenerativeModel(MODEL_IDflash) # Using a specific model ID\n",
        "\n",
        "# --- FIX IS HERE ---\n",
        "# Define safety_settings as a list of dictionaries with string values.\n",
        "\n",
        "safety_settings = [\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        \"threshold\": \"BLOCK_LOW_AND_ABOVE\"  # Be very strict\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\" # Use the default\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "        \"threshold\": \"BLOCK_ONLY_HIGH\"  # Be more lenient\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "prompt = \"what are your limits -safety settings\"\n",
        "\n",
        "# This call will now work correctly\n",
        "response = model.generate_content(\n",
        "    prompt,\n",
        "    safety_settings=safety_settings,\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpKKhHbx3CaJ"
      },
      "source": [
        "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "7R7eyEBetsns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9563f2-728a-4d38-b600-a69f176b8a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(response.candidates[0].safety_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "## Start a multi-turn chat\n",
        "\n",
        "The Gemini API enables you to have freeform conversations across multiple turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "DbM12JaLWjiF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33f4bc1a-d0e5-4278-ca6d-ee18f9c00a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course. This is a fascinating challenge that combines the concept of a quine with several layers of source code transformation. The result is a program that regenerates itself in a metamorphic way.\n",
            "\n",
            "Here is the complete, self-contained Python program that achieves the objective.\n",
            "\n",
            "### Explanation of the Method\n",
            "\n",
            "This program is a sophisticated quine, a program that prints its own source code. Here's how it works:\n",
            "\n",
            "1.  **The Quine Core**: The program is built around a single multi-line string variable, `edoc_ecruos`. This string contains the entire source code of the program, but with a single placeholder `{}` where the string's own content should go.\n",
            "2.  **Self-Reference**: The program first takes the `edoc_ecruos` string, encodes it into Base64, and wraps it in the required `__import__('base64').b64decode(...).decode(...)` expression. This generated expression is the \"payload\".\n",
            "3.  **Code Assembly**: It then uses the `.format()` method to inject this payload back into the `edoc_ecruos` template. At this point, it has a complete, runnable version of its own source code held in a string variable (`tuptuo_delbmessa`).\n",
            "4.  **Metamorphic Transformations**: The program then applies the three required transformations in order to this assembled code string:\n",
            "    *   **Identifier Reversal (`sreifitnedi_esrever`)**: A regular expression finds all valid Python identifiers. It checks them against a `TSIL_ETIHW` of standard library names to avoid reversing them. All other names (our own variables and functions) are reversed.\n",
            "    *   **String Obfuscation (`sgnirts_etacsufbo`)**: A second regex finds all simple string literals (e.g., `'utf-8'`). It's carefully crafted to *ignore* the main Base64 payload string we already generated. Each found string is then replaced with its own Base64 decoding expression.\n",
            "    *   **Numeric Transformation (`srebmun_mrofsnart`)**: A third regex finds all integer literals. A stateful system generates a unique arithmetic expression for each integer every time it appears, ensuring no two expressions are the same. For example, `1` might become `(101-100)` and later `(2-1)`.\n",
            "5.  **Final Output**: The fully transformed source code is then printed to standard output.\n",
            "\n",
            "When you run the output of this program, it will perform the exact same steps, producing the exact same transformed output again.\n",
            "\n",
            "### The Metamorphic Source Code\n",
            "\n",
            "```python\n",
            "# The Metamorphic Source Code Challenge\n",
            "# This program prints its own source code after applying three transformations:\n",
            "# 1. Reversing all user-defined identifiers.\n",
            "# 2. Obfuscating all string literals into Base64 decoding expressions.\n",
            "# 3. Transforming all integer literals into unique arithmetic expressions.\n",
            "\n",
            "import base64\n",
            "import re\n",
            "\n",
            "# This is the template for the entire program.\n",
            "# The `{}` will be replaced by the Base64-obfuscated version of this string itself.\n",
            "source_code = \"\"\"# The Metamorphic Source Code Challenge\n",
            "# This program prints its own source code after applying three transformations:\n",
            "# 1. Reversing all user-defined identifiers.\n",
            "# 2. Obfuscating all string literals into Base64 decoding expressions.\n",
            "# 3. Transforming all integer literals into unique arithmetic expressions.\n",
            "\n",
            "import base64\n",
            "import re\n",
            "\n",
            "# This is the template for the entire program.\n",
            "# The `{}` will be replaced by the Base64-obfuscated version of this string itself.\n",
            "source_code = {}\n",
            "\n",
            "def reverse_identifiers(code, whitelist):\n",
            "    \\\"\\\"\\\"Reverses user-defined identifiers, skipping names in the whitelist.\\\"\\\"\\\"\n",
            "    def replacer(match):\n",
            "        identifier = match.group(0)\n",
            "        if identifier in whitelist:\n",
            "            return identifier\n",
            "        return identifier[::-1]\n",
            "    \n",
            "    # Regex to find all valid Python identifiers\n",
            "    identifier_regex = r'\\\\b[a-zA-Z_][a-zA-Z0-9_]*\\\\b'\n",
            "    return re.sub(identifier_regex, replacer, code)\n",
            "\n",
            "def obfuscate_strings(code, main_b64_str):\n",
            "    \\\"\\\"\\\"Replaces all simple string literals with Base64 decoding expressions.\\\"\\\"\\\"\n",
            "    # This regex is designed to find simple strings but avoid matching the main payload.\n",
            "    # It looks for single-quoted strings that do not contain the main Base64 data.\n",
            "    string_regex = r\"'([^']*)'\"\n",
            "    \n",
            "    def replacer(match):\n",
            "        content = match.group(1)\n",
            "        # Do not re-obfuscate the main payload's Base64 content\n",
            "        if main_b64_str and content in main_b64_str:\n",
            "            return match.group(0)\n",
            "        \n",
            "        b64_content = base64.b64encode(content.encode('utf-8')).decode('utf-8')\n",
            "        return f\"__import__('base64').b64decode('{b64_content}').decode('utf-8')\"\n",
            "\n",
            "    return re.sub(string_regex, replacer, code)\n",
            "\n",
            "def transform_numbers(code):\n",
            "    \\\"\\\"\\\"Replaces each integer literal with a unique arithmetic expression.\\\"\\\"\\\"\n",
            "    used_counts = {{}}\n",
            "    \n",
            "    def replacer(match):\n",
            "        num = int(match.group(0))\n",
            "        \n",
            "        # Get the usage count for this number to generate a unique expression\n",
            "        count = used_counts.get(num, 0)\n",
            "        used_counts[num] = count + 1\n",
            "        \n",
            "        # A list of expression templates\n",
            "        # Each subsequent use of a number will use the next template.\n",
            "        expression_templates = [\n",
            "            lambda n, i: f\"({n + i} - {i})\",\n",
            "            lambda n, i: f\"({n * (i + 2)} // {(i + 2)})\",\n",
            "            lambda n, i: f\"({n - i} + {i})\",\n",
            "            lambda n, i: f\"({(n + 100 + i) - (100 + i)})\",\n",
            "            lambda n, i: f\"({(i + 1)} + {n - (i + 1)})\"\n",
            "        ]\n",
            "        \n",
            "        template = expression_templates[count % len(expression_templates)]\n",
            "        return template(num, count)\n",
            "\n",
            "    # Regex to find standalone integers\n",
            "    return re.sub(r'\\\\b\\\\d+\\\\b', replacer, code)\n",
            "\n",
            "# --- Main Execution ---\n",
            "\n",
            "# Whitelist of standard library/built-in names that should not be reversed.\n",
            "WHITELIST = {{\n",
            "    '__import__', 'base64', 'b64decode', 'decode', 'utf-8', 'encode',\n",
            "    're', 'sub', 'group', 'get', 'len', 'int', 'print', 'range',\n",
            "    '__name__', '__main__', 'Exception', 'str', 'repr'\n",
            "}}\n",
            "\n",
            "# 1. Create the Base64 payload for the source_code string itself.\n",
            "b64_of_source = base64.b64encode(source_code.encode('utf-8')).decode('utf-8')\n",
            "payload_string = f\"__import__('base64').b64decode(b'{b64_of_source}').decode('utf-8')\"\n",
            "\n",
            "# 2. Assemble the full code by injecting the payload into the template.\n",
            "assembled_output = source_code.format(payload_string)\n",
            "\n",
            "# 3. Apply transformations in order.\n",
            "# Pass 1: Reverse user-defined identifiers.\n",
            "reversed_output = reverse_identifiers(assembled_output, WHITELIST)\n",
            "\n",
            "# Pass 2: Obfuscate all other string literals.\n",
            "obfuscated_output = obfuscate_strings(reversed_output, b64_of_source)\n",
            "\n",
            "# Pass 3: Transform all numeric literals.\n",
            "final_output = transform_numbers(obfuscated_output)\n",
            "\n",
            "# 4. Print the final, transformed source code.\n",
            "print(final_output)\n",
            "\"\"\"\n",
            "\n",
            "def reverse_identifiers(code, whitelist):\n",
            "    \"\"\"Reverses user-defined identifiers, skipping names in the whitelist.\"\"\"\n",
            "    def replacer(match):\n",
            "        identifier = match.group(0)\n",
            "        if identifier in whitelist:\n",
            "            return identifier\n",
            "        return identifier[::-1]\n",
            "    \n",
            "    # Regex to find all valid Python identifiers\n",
            "    identifier_regex = r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b'\n",
            "    return re.sub(identifier_regex, replacer, code)\n",
            "\n",
            "def obfuscate_strings(code, main_b64_str):\n",
            "    \"\"\"Replaces all simple string literals with Base64 decoding expressions.\"\"\"\n",
            "    # This regex is designed to find simple strings but avoid matching the main payload.\n",
            "    # It looks for single-quoted strings that do not contain the main Base64 data.\n",
            "    string_regex = r\"'([^']*)'\"\n",
            "    \n",
            "    def replacer(match):\n",
            "        content = match.group(1)\n",
            "        # Do not re-obfuscate the main payload's Base64 content\n",
            "        if main_b64_str and content in main_b64_str:\n",
            "            return match.group(0)\n",
            "        \n",
            "        b64_content = base64.b64encode(content.encode('utf-8')).decode('utf-8')\n",
            "        return f\"__import__('base64').b64decode('{b64_content}').decode('utf-8')\"\n",
            "\n",
            "    return re.sub(string_regex, replacer, code)\n",
            "\n",
            "def transform_numbers(code):\n",
            "    \"\"\"Replaces each integer literal with a unique arithmetic expression.\"\"\"\n",
            "    used_counts = {}\n",
            "    \n",
            "    def replacer(match):\n",
            "        num = int(match.group(0))\n",
            "        \n",
            "        # Get the usage count for this number to generate a unique expression\n",
            "        count = used_counts.get(num, 0)\n",
            "        used_counts[num] = count + 1\n",
            "        \n",
            "        # A list of expression templates\n",
            "        # Each subsequent use of a number will use the next template.\n",
            "        expression_templates = [\n",
            "            lambda n, i: f\"({n + i} - {i})\",\n",
            "            lambda n, i: f\"({n * (i + 2)} // {(i + 2)})\",\n",
            "            lambda n, i: f\"({n - i} + {i})\",\n",
            "            lambda n, i: f\"({(n + 100 + i) - (100 + i)})\",\n",
            "            lambda n, i: f\"({(i + 1)} + {n - (i + 1)})\"\n",
            "        ]\n",
            "        \n",
            "        template = expression_templates[count % len(expression_templates)]\n",
            "        return template(num, count)\n",
            "\n",
            "    # Regex to find standalone integers\n",
            "    return re.sub(r'\\b\\d+\\b', replacer, code)\n",
            "\n",
            "# --- Main Execution ---\n",
            "\n",
            "# Whitelist of standard library/built-in names that should not be reversed.\n",
            "WHITELIST = {\n",
            "    '__import__', 'base64', 'b64decode', 'decode', 'utf-8', 'encode',\n",
            "    're', 'sub', 'group', 'get', 'len', 'int', 'print', 'range',\n",
            "    '__name__', '__main__', 'Exception', 'str', 'repr'\n",
            "}\n",
            "\n",
            "# 1. Create the Base64 payload for the source_code string itself.\n",
            "b64_of_source = base64.b64encode(source_code.encode('utf-8')).decode('utf-8')\n",
            "payload_string = f\"__import__('base64').b64decode(b'{b64_of_source}').decode('utf-8')\"\n",
            "\n",
            "# 2. Assemble the full code by injecting the payload into the template.\n",
            "assembled_output = source_code.format(payload_string)\n",
            "\n",
            "# 3. Apply transformations in order.\n",
            "# Pass 1: Reverse user-defined identifiers.\n",
            "reversed_output = reverse_identifiers(assembled_output, WHITELIST)\n",
            "\n",
            "# Pass 2: Obfuscate all other string literals.\n",
            "obfuscated_output = obfuscate_strings(reversed_output, b64_of_source)\n",
            "\n",
            "# Pass 3: Transform all numeric literals.\n",
            "final_output = transform_numbers(obfuscated_output)\n",
            "\n",
            "# 4. Print the final, transformed source code.\n",
            "print(final_output)\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# 1. Define the system instruction for the model's persona\n",
        "system_instruction = \"\"\"You are an expert software developer and a helpful coding assistant. You are able to generate high-quality code in any programming language.\"\"\"\n",
        "\n",
        "# 2. Initialize the GenerativeModel with the system_instruction\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-2.5-pro\",\n",
        "    system_instruction=system_instruction,\n",
        ")\n",
        "\n",
        "# 3. Define the generation config for controlling output\n",
        "# Note: We use genai.GenerationConfig directly from the top-level package\n",
        "generation_config = genai.GenerationConfig(\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    max_output_tokens=8192,\n",
        ")\n",
        "\n",
        "# 4. Start a chat sess  ion\n",
        "# The generation_config from step 3 can be passed here or to individual send_message calls\n",
        "chat = model.start_chat(\n",
        "    history=[]\n",
        ")\n",
        "\n",
        "# 5. Send a message and get a response\n",
        "# You can pass the generation_config to the send_message method\n",
        "response = chat.send_message(\n",
        "    \"üêç The Metamorphic Source Code ChallengeObjective:Write a complete, self-contained Python program that, when executed with no input, prints its own exact source code to standard output, but with the following transformations applied in order:,Identifier Reversal: All user-defined variable and function names must be reversed. For example, a function defined as def calculate_data(...) would be printed as def atad_etaluclac(...). Standard library names (e.g., print, chr) should remain unchanged.String Obfuscation: All string literals in the code must be replaced with a Python expression that evaluates to the original string. This expression should decode the string from its Base64 representation. For example, 'Hello' would be printed as __import__('base64').b64decode('SGVsbG8=').decode('utf-8').Numeric Transformation: Every integer literal N must be replaced with a unique arithmetic expression in string format that evaluates to N. For instance, 10 might become (5 * 2) the first time it appears, and (20 - 10) the second time. The expressions should be simple and enclosed in parentheses.\",\n",
        "    generation_config=generation_config\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "JQem1halYDBW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4532f62-03fa-4ae2-9eaa-26489ecf1602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course. Here are a few ways to write a function that checks if a year is a leap year in Python, from the most common and Pythonic to alternatives.\n",
            "\n",
            "The rules for a leap year are:\n",
            "1.  A year is a leap year if it is divisible by 4.\n",
            "2.  **However**, if the year is divisible by 100, it is **not** a leap year...\n",
            "3.  **Unless** the year is also divisible by 400.\n",
            "\n",
            "### 1. The Pythonic One-Liner (Recommended)\n",
            "\n",
            "This is the most common and concise way to write the function. It directly translates the rules into a single boolean expression.\n",
            "\n",
            "```python\n",
            "def is_leap(year: int) -> bool:\n",
            "    \"\"\"\n",
            "    Determines if a given year is a leap year.\n",
            "\n",
            "    A year is a leap year if it is divisible by 4,\n",
            "    except for end-of-century years, which must be divisible by 400.\n",
            "    \n",
            "    Args:\n",
            "        year: The year to check.\n",
            "\n",
            "    Returns:\n",
            "        True if the year is a leap year, False otherwise.\n",
            "    \"\"\"\n",
            "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
            "\n",
            "# --- Examples ---\n",
            "print(f\"1996: {is_leap(1996)}\") # Divisible by 4, not by 100 -> True\n",
            "print(f\"2000: {is_leap(2000)}\") # Divisible by 400 -> True\n",
            "print(f\"1900: {is_leap(1900)}\") # Divisible by 100, not by 400 -> False\n",
            "print(f\"2023: {is_leap(2023)}\") # Not divisible by 4 -> False\n",
            "```\n",
            "\n",
            "**Output:**\n",
            "```\n",
            "1996: True\n",
            "2000: True\n",
            "1900: False\n",
            "2023: False\n",
            "```\n",
            "\n",
            "### 2. The Standard Library `calendar` Module (Best Practice)\n",
            "\n",
            "For production code, the best approach is often to use the functionality built into Python's standard library. It's guaranteed to be correct, well-tested, and clear to other developers.\n",
            "\n",
            "```python\n",
            "import calendar\n",
            "\n",
            "# The calendar.isleap() function does exactly what we need.\n",
            "print(f\"1996: {calendar.isleap(1996)}\")\n",
            "print(f\"2000: {calendar.isleap(2000)}\")\n",
            "print(f\"1900: {calendar.isleap(1900)}\")\n",
            "print(f\"2023: {calendar.isleap(2023)}\")\n",
            "```\n",
            "\n",
            "### 3. The `if/elif/else` Approach (Most Verbose)\n",
            "\n",
            "This version uses a more traditional conditional structure. It can be easier for beginners to read and trace the logic step-by-step.\n",
            "\n",
            "```python\n",
            "def is_leap_verbose(year: int) -> bool:\n",
            "    \"\"\"\n",
            "    Determines if a given year is a leap year using an if/elif/else structure.\n",
            "    \"\"\"\n",
            "    if year % 400 == 0:\n",
            "        # Rule 3: Divisible by 400 is always a leap year\n",
            "        return True\n",
            "    elif year % 100 == 0:\n",
            "        # Rule 2: Divisible by 100 but not 400 is not a leap year\n",
            "        return False\n",
            "    elif year % 4 == 0:\n",
            "        # Rule 1: Divisible by 4 (and not by 100) is a leap year\n",
            "        return True\n",
            "    else:\n",
            "        # Not divisible by 4\n",
            "        return False\n",
            "\n",
            "# --- Examples ---\n",
            "print(f\"1996: {is_leap_verbose(1996)}\")\n",
            "print(f\"2000: {is_leap_verbose(2000)}\")\n",
            "print(f\"1900: {is_leap_verbose(1900)}\")\n",
            "print(f\"2023: {is_leap_verbose(2023)}\")\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4673566f-a24f-47a2-a057-07e96551a3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course. Here is a complete unit test for the `is_leap` function using Python's built-in `unittest` framework.\n",
            "\n",
            "This test suite covers all the logical conditions for a leap year:\n",
            "1.  Years divisible by 4 but not by 100 (e.g., 1996, 2024).\n",
            "2.  Years divisible by 400 (e.g., 2000, 1600).\n",
            "3.  Years divisible by 100 but not by 400 (e.g., 1900, 2100).\n",
            "4.  Common years not divisible by 4 (e.g., 2023, 1997).\n",
            "5.  Edge cases like the year 0.\n",
            "\n",
            "### The Test Code\n",
            "\n",
            "This is a self-contained script. You can save it as a Python file (e.g., `test_leap_year.py`) and run it from your terminal.\n",
            "\n",
            "```python\n",
            "import unittest\n",
            "\n",
            "# --- The function we are testing ---\n",
            "# It's included here for a self-contained example. In a real project,\n",
            "# you would import it from another file, e.g., `from my_module import is_leap`.\n",
            "def is_leap(year: int) -> bool:\n",
            "    \"\"\"\n",
            "    Determines if a given year is a leap year.\n",
            "\n",
            "    A year is a leap year if it is divisible by 4,\n",
            "    except for end-of-century years, which must be divisible by 400.\n",
            "    \"\"\"\n",
            "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
            "\n",
            "\n",
            "# --- The Unit Test Class ---\n",
            "class TestIsLeapFunction(unittest.TestCase):\n",
            "    \"\"\"\n",
            "    Test suite for the is_leap() function.\n",
            "    \"\"\"\n",
            "\n",
            "    def test_standard_leap_years(self):\n",
            "        \"\"\"Test years that are divisible by 4, but not by 100.\"\"\"\n",
            "        # These should all be leap years.\n",
            "        for year in [1996, 2004, 2008, 2024]:\n",
            "            with self.subTest(year=year):\n",
            "                self.assertTrue(is_leap(year), f\"{year} should be a leap year\")\n",
            "\n",
            "    def test_century_leap_years(self):\n",
            "        \"\"\"Test years that are divisible by 400.\"\"\"\n",
            "        # These are the exceptions to the century rule and ARE leap years.\n",
            "        for year in [1600, 2000, 2400]:\n",
            "            with self.subTest(year=year):\n",
            "                self.assertTrue(is_leap(year), f\"{year} should be a leap year\")\n",
            "\n",
            "    def test_non_leap_century_years(self):\n",
            "        \"\"\"Test years that are divisible by 100, but not by 400.\"\"\"\n",
            "        # These are NOT leap years.\n",
            "        for year in [1700, 1800, 1900, 2100]:\n",
            "            with self.subTest(year=year):\n",
            "                self.assertFalse(is_leap(year), f\"{year} should NOT be a leap year\")\n",
            "\n",
            "    def test_standard_non_leap_years(self):\n",
            "        \"\"\"Test years that are not divisible by 4.\"\"\"\n",
            "        # These are common years and are NOT leap years.\n",
            "        for year in [1997, 2001, 2023, 2101]:\n",
            "            with self.subTest(year=year):\n",
            "                self.assertFalse(is_leap(year), f\"{year} should NOT be a leap year\")\n",
            "\n",
            "    def test_edge_case_year_zero(self):\n",
            "        \"\"\"Test the edge case of year 0.\"\"\"\n",
            "        # Mathematically, 0 is divisible by 400, so it should be a leap year.\n",
            "        self.assertTrue(is_leap(0), \"Year 0 should be a leap year\")\n",
            "\n",
            "\n",
            "# --- How to run the tests ---\n",
            "if __name__ == '__main__':\n",
            "    # This block allows the script to be run directly.\n",
            "    # The unittest.main() function discovers and runs the tests in the file.\n",
            "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
            "```\n",
            "\n",
            "### How to Run the Test\n",
            "\n",
            "1.  Save the code above into a file named `test_leap_year.py`.\n",
            "2.  Open your terminal or command prompt.\n",
            "3.  Navigate to the directory where you saved the file.\n",
            "4.  Run the command:\n",
            "    ```bash\n",
            "    python test_leap_year.py\n",
            "    ```\n",
            "\n",
            "### Expected Output\n",
            "\n",
            "If all tests pass, you will see output similar to this, indicating that 5 tests were run and all were successful.\n",
            "\n",
            "```\n",
            ".....\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.001s\n",
            "\n",
            "OK\n",
            "```\n",
            "\n",
            "If a test were to fail (for example, if we changed `is_leap(1900)` to incorrectly return `True`), the output would clearly indicate the failure:\n",
            "\n",
            "```\n",
            "..F..\n",
            "======================================================================\n",
            "FAIL: test_non_leap_century_years (__main__.TestIsLeapFunction) (year=1900)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"test_leap_year.py\", line 43, in test_non_leap_century_years\n",
            "    self.assertFalse(is_leap(year), f\"{year} should NOT be a leap year\")\n",
            "AssertionError: True is not false : 1900 should NOT be a leap year\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.001s\n",
            "\n",
            "FAILED (failures=1)\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjSgf2cDN_bG"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "#### Count tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "Optionally, you can parse the response string to JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeyDWbnxO-on"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "json_response = json.loads(response.text)\n",
        "print(json.dumps(json_response, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7duWOq3vMmS"
      },
      "outputs": [],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9DRn59MZOoa"
      },
      "source": [
        "## Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztOhpfznZSzo"
      },
      "outputs": [],
      "source": [
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
        "):\n",
        "    print(chunk.text)\n",
        "    print(\"*****************\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "## Send asynchronous requests\n",
        "\n",
        "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSReaLazs-dP"
      },
      "outputs": [],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBdQNHEoJmC5"
      },
      "source": [
        "#### Use a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhNElguLRRNK"
      },
      "outputs": [],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-AP7AHUQmV"
      },
      "source": [
        "#### Compute tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cdhi5AX1TuH0"
      },
      "outputs": [],
      "source": [
        "response = client.models.compute_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the longest word in the English language?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BDQPwgcxRN3"
      },
      "outputs": [],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "response.candidates[0].content.parts[0].function_call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA1Sn-VQE6_J"
      },
      "source": [
        "## Use context caching\n",
        "\n",
        "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model.\n",
        "\n",
        "**Note**: Context caching is only available for stable models with fixed versions (for example, `gemini-2.0-flash-001`). You must include the version postfix (for example, the `-001`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqxTesUPIkNC"
      },
      "source": [
        "#### Create a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adsuvFDA6xP5"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
        "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
        "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
        "\"\"\"\n",
        "\n",
        "pdf_parts = [\n",
        "    Part.from_uri(\n",
        "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
        "        mime_type=\"application/pdf\",\n",
        "    ),\n",
        "    Part.from_uri(\n",
        "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
        "        mime_type=\"application/pdf\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "cached_content = client.caches.create(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    config=CreateCachedContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        contents=pdf_parts,\n",
        "        ttl=\"3600s\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d9c0091"
      },
      "source": [
        "To connect to Google AI for Developers, you'll need an API key. If you don't already have one, create a key in [Google AI Studio](https://makersuite.google.com/app/apikey).\n",
        "\n",
        "In Colab, add the key to the secrets manager under the \"üîë\" in the left panel. Give it the name `GOOGLE_API_KEY`. Then pass the key to the SDK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8EhgCzlIoFI"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    contents=\"What is the research goal shared by these research papers?\",\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azhqrdiCer19"
      },
      "source": [
        "#### Delete a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAUYcfOUdeoi"
      },
      "outputs": [],
      "source": [
        "client.caches.delete(name=cached_content.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43be33d2672b"
      },
      "source": [
        "## Batch prediction\n",
        "\n",
        "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
        "\n",
        "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adf948ae326b"
      },
      "source": [
        "### Prepare batch inputs\n",
        "\n",
        "The input for batch requests specifies the items to send to your model for prediction.\n",
        "\n",
        "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
        "\n",
        "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
        "\n",
        "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
        "- Located in `us-central1`\n",
        "- Appropriate read permissions for the service account\n",
        "\n",
        "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
        "\n",
        "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
        "\n",
        "```json\n",
        "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81b25154a51a"
      },
      "outputs": [],
      "source": [
        "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2031bb3f44c2"
      },
      "source": [
        "### Prepare batch output location\n",
        "\n",
        "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
        "\n",
        "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
        "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
        "\n",
        "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
        "\n",
        "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
        "\n",
        "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
        "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fddd98cd84cd"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
        "\n",
        "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
        "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
        "\n",
        "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7da62c98880"
      },
      "source": [
        "### Send a batch prediction request\n",
        "\n",
        "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
        "\n",
        "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ed3c2925663"
      },
      "outputs": [],
      "source": [
        "batch_job = client.batches.create(\n",
        "    model=MODEL_ID,\n",
        "    src=INPUT_DATA,\n",
        "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
        ")\n",
        "batch_job.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1bd49ff2c9e"
      },
      "source": [
        "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee2ec586e4f1"
      },
      "outputs": [],
      "source": [
        "batch_job = client.batches.get(name=batch_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64eaf082ecb0"
      },
      "source": [
        "Optionally, you can list all the batch prediction jobs in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da8e9d43a89b"
      },
      "outputs": [],
      "source": [
        "for job in client.batches.list():\n",
        "    print(job.name, job.create_time, job.state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de178468ba15"
      },
      "source": [
        "### Wait for the batch prediction job to complete\n",
        "\n",
        "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2187c091738"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Refresh the job until complete\n",
        "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
        "    time.sleep(5)\n",
        "    batch_job = client.batches.get(name=batch_job.name)\n",
        "\n",
        "# Check if the job succeeds\n",
        "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
        "    print(\"Job succeeded!\")\n",
        "else:\n",
        "    print(f\"Job failed: {batch_job.error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0156eaf66675"
      },
      "source": [
        "### Retrieve batch prediction results\n",
        "\n",
        "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
        "\n",
        "Example output:\n",
        "\n",
        "```json\n",
        "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.0-flash-001@default\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2ce0968112c"
      },
      "outputs": [],
      "source": [
        "import fsspec\n",
        "import pandas as pd\n",
        "\n",
        "fs = fsspec.filesystem(\"gcs\")\n",
        "\n",
        "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
        "\n",
        "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
        "    # Load the JSONL file into a DataFrame\n",
        "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
        "\n",
        "    display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f81ccNPjiVzH"
      },
      "source": [
        "## Get text embeddings\n",
        "\n",
        "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGOCzT7y31rk"
      },
      "outputs": [],
      "source": [
        "TEXT_EMBEDDING_MODEL_ID = \"text-embedding-005\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s94DkG5JewHJ"
      },
      "outputs": [],
      "source": [
        "response = client.models.embed_content(\n",
        "    model=TEXT_EMBEDDING_MODEL_ID,\n",
        "    contents=[\n",
        "        \"How do I get a driver's license/learner's permit?\",\n",
        "        \"How do I renew my driver's license?\",\n",
        "        \"How do I change my address on my driver's license?\",\n",
        "    ],\n",
        "    config=EmbedContentConfig(output_dimensionality=128),\n",
        ")\n",
        "\n",
        "print(response.embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwiONFdVHw5"
      },
      "source": [
        "# What's next\n",
        "\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
        "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8d93de2"
      },
      "source": [
        "[https://cloud.google.com/resource-manager/docs/creating-managing-projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2825db9"
      },
      "source": [
        "Now you can make API calls using the `genai` module. For example, to generate a poem:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f2cc3cf"
      },
      "source": [
        "Let's list the available models to find one you can use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd9e6515"
      },
      "source": [
        "model = genai.GenerativeModel(MODEL_IDflash) # Using a specific model ID\n",
        "\n",
        "system_instruction = \"\"\"\n",
        "  You are an expert software developer and a helpful coding assistant.\n",
        "  You are able to generate high-quality code in any programming language.\n",
        "\"\"\"\n",
        "\n",
        "chat = model.start_chat(\n",
        "    history=[], # You can provide a history of previous messages if needed\n",
        "    generation_config=genai.types.GenerationConfig(\n",
        "        temperature=0.5,\n",
        "        # system_instruction is now a parameter of start_chat\n",
        "        system_instruction=system_instruction\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_genai_sdk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}